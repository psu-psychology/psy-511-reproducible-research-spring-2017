2017-01-23-manifesto
================
Rick Gilmore
2017-02-12 14:48:38

"A manifesto for reproducible science"
======================================

Discussion of
-------------

Munafò, M. R., Nosek, B. A., Bishop, D. V. M., Button, K. S., Chambers, C. D., Sert, N. P. du, … Ioannidis, J. P. A. (2017). A manifesto for reproducible science. Nature Human Behaviour, 1, 0021. <https://doi.org/10.1038/s41562-016-0021>.

Steps in scientific method (and weaknesses)
-------------------------------------------

-   Generate and specify hypothesis
-   Design study
-   Conduct study and collect data
-   Analyze data and test hypothesis
-   Interpret results
-   Publish and/or conduct next study

------------------------------------------------------------------------

<a href="http://www.nature.com/articles/s41562-016-0021/figures/1"> <img src="http://www.nature.com/article-assets/npg/nathumbehav/2017/s41562-016-0021/images_hires/w926/s41562-016-0021-f1.jpg" height=500px> </a>

Failure to control for bias
---------------------------

-   [*Apophenia*](https://en.wikipedia.org/wiki/Apophenia)
-   Confirmation bias
-   Hindsight bias
-   [Kahneman, D. (2011). Thinking Fast and Slow. Farrar, Straus, and Giroux.](https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow)

Low statistical power
---------------------

-   Button, K. S., Ioannidis, J. P. A., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S. J., & Munafò, M. R. (2013). Power failure: why small sample size undermines the reliability of neuroscience. Nature Reviews Neuroscience, 14(5), 365–376. <https://doi.org/10.1038/nrn3475>
-   Ioannidis, J. P. A. (2005). Why Most Published Research Findings Are False. PLOS Medicine, 2(8), e124. <https://doi.org/10.1371/journal.pmed.0020124>

Low statistical power
---------------------

-   Szucs, D., & Ioannidis, J. P. (2016). Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature. bioRxiv, 071530. <https://doi.org/10.1101/071530>

Poor quality control
--------------------

-   Goodman, S. N., Fanelli, D., & Ioannidis, J. P. A. (2016). What does research reproducibility mean? Science Translational Medicine, 8(341), 341ps12–341ps12. <https://doi.org/10.1126/scitranslmed.aaf5027>
-   *Methods* reproducibility
    -   *"…the ability to implement, as exactly as possible, the experimental and computational procedures, with the same data and tools, to obtain the same results."*

P-Hacking
---------

-   Simonsohn, U., Nelson, L. D., & Simmons, J. P. (2014). P-curve: A key to the file-drawer. Journal of Experimental Psychology: General, 143(2), 534–547. <https://doi.org/10.1037/a0033242>
-   If an effect is *true*, the distribution of reported *p* values should be right-skewed (long right tail)
-   <http://www.p-curve.com/>
-   [p-curve app](http://www.p-curve.com/app4/)

HARKing: hypothesizing after the results are known
--------------------------------------------------

-   Kerr, N. L. (1998). HARKing: Hypothesizing After the Results are Known. Personality and Social Psychology Review, 2(3), 196–217. <https://doi.org/10.1207/s15327957pspr0203_4>
-   Find an effect in data analysis
-   Present effect as if it had been hypothesized

Publication bias
----------------

-   Results vs. null findings
-   Novel results vs. replications
-   Counter-intuitive findings
-   [File drawer effect](http://www.psychfiledrawer.org/TheFiledrawerProblem.php)
    -   How many unpublished failures to replicate sit in file drawers?

Overcoming these weaknesses
===========================

Performing research
-------------------

-   Protecting against cognitive biases
-   Improving methodological training
-   Implementing independent methological support
-   Encouraging collaboration and team science
-   Collect bigger samples

Reporting on research
---------------------

-   Promoting study pre-registration
    -   Registered reports ([Munafo et al. 2017](https://doi.org/10.1038/s41562-016-0021), Box 3)
-   Improving the quality of reporting
    -   [The Transparency and Openness Promotion (TOP) guidelines](https://osf.io/ud578/) and [signatories](https://cos.io/top/#list)

Reporting on research
---------------------

-   Franco, A., Malhotra, N., & Simonovits, G. (2016). Underreporting in Psychology Experiments: Evidence From a Study Registry. Social Psychological and Personality Science, 7(1), 8–12. <https://doi.org/10.1177/1948550615598377>
-   "*We find that about 40% of studies fail to fully report all experimental conditions and about 70% of studies do not report all outcome variables included in the questionnaire. Reported effect sizes are about twice as large as unreported effect sizes and are about 3 times more likely to be statistically significant.*"

Reporting on research
---------------------

-   Publish replications
-   Frank, M. C., & Saxe, R. (2012). Teaching Replication. Perspectives on Psychological Science, 7(6), 600–604. <https://doi.org/10.1177/1745691612460686>.

Verifying research
------------------

-   Promoting transparency and open science
-   Open methods, materials, code sharing, data sharing,

Changing Incentives
-------------------

-   Higginson, A. D., & Munafò, M. R. (2016). Current Incentives for Scientists Lead to Underpowered Studies with Erroneous Conclusions. PLOS Biology, 14(11), e2000995. <https://doi.org/10.1371/journal.pbio.2000995>
-   Claim that current publication incentive structure reinforces current practices
-   [OSF badge system](https://osf.io/tvyxz/)
-   Other incentives/disincentives

Status report/recommendations by stakeholder group
--------------------------------------------------

<iframe src="http://www.nature.com/articles/s41562-016-0021/tables/1">
</iframe>
Source: <http://www.nature.com/articles/s41562-016-0021/tables/1>

Your thoughts?
==============

Questions for discussion
------------------------

-   Which of the manifesto provisions would you disagree with?
-   Do you agree with the assessment about progress ([Table 1](http://www.nature.com/articles/s41562-016-0021/tables/1))
-   What steps could **you** take?
