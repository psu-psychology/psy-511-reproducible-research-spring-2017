---
title: "2017-01-23-manifesto"
author: "Rick Gilmore"
date: "`r Sys.time()`"
bibliography: ../bib/reproducibility.bib
always_allow_html: yes
output:
  ioslides_presentation:
    css: ../css/gilmore-ioslides.css
    widescreen: true
    incremental: false
    transition: default
---

# "A manifesto for reproducible science"

## Discussion of

Munafò, M. R., Nosek, B. A., Bishop, D. V. M., Button, K. S., Chambers, C. D., Sert, N. P. du, … Ioannidis, J. P. A. (2017). A manifesto for reproducible science. Nature Human Behaviour, 1, 0021. <https://doi.org/10.1038/s41562-016-0021>.

## Steps in scientific method (and weaknesses)

- Generate and specify hypothesis
- Design study
- Conduct study and collect data
- Analyze data and test hypothesis
- Interpret results
- Publish and/or conduct next study

---

<div class="centered">
<a href="http://www.nature.com/articles/s41562-016-0021/figures/1">
<img src="http://www.nature.com/article-assets/npg/nathumbehav/2017/s41562-016-0021/images_hires/w926/s41562-016-0021-f1.jpg" height=500px>
</a>
</div>

## Failure to control for bias

- [*Apophenia*](https://en.wikipedia.org/wiki/Apophenia)
- Confirmation bias
- Hindsight bias
- [Kahneman, D. (2011). Thinking Fast and Slow. Farrar, Straus, and Giroux.](https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow)

## Low statistical power

- Button, K. S., Ioannidis, J. P. A., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S. J., & Munafò, M. R. (2013). Power failure: why small sample size undermines the reliability of neuroscience. Nature Reviews Neuroscience, 14(5), 365–376. <https://doi.org/10.1038/nrn3475>
- Ioannidis, J. P. A. (2005). Why Most Published Research Findings Are False. PLOS Medicine, 2(8), e124. <https://doi.org/10.1371/journal.pmed.0020124>

## Low statistical power

- Szucs, D., & Ioannidis, J. P. (2016). Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature. bioRxiv, 071530. <https://doi.org/10.1101/071530>

## Poor quality control

- Goodman, S. N., Fanelli, D., & Ioannidis, J. P. A. (2016). What does research reproducibility mean? Science Translational Medicine, 8(341), 341ps12–341ps12. <https://doi.org/10.1126/scitranslmed.aaf5027>
- *Methods* reproducibility
    - *"…the ability to implement, as exactly as possible, the experimental and computational procedures, with the same data and tools, to obtain the same results."*

## P-Hacking

- Simonsohn, U., Nelson, L. D., & Simmons, J. P. (2014). P-curve: A key to the file-drawer. Journal of Experimental Psychology: General, 143(2), 534–547. <https://doi.org/10.1037/a0033242>
- If an effect is *true*, the distribution of reported *p* values should be right-skewed (long right tail)
- <http://www.p-curve.com/>
- [p-curve app](http://www.p-curve.com/app4/)

## HARKing: hypothesizing after the results are known

- Kerr, N. L. (1998). HARKing: Hypothesizing After the Results are Known. Personality and Social Psychology Review, 2(3), 196–217. <https://doi.org/10.1207/s15327957pspr0203_4>
- Find an effect in data analysis
- Present effect as if it had been hypothesized

## Publication bias

- Results vs. null findings
- Novel results vs. replications
- Counter-intuitive findings
- [File drawer effect](http://www.psychfiledrawer.org/TheFiledrawerProblem.php)
    - How many unpublished failures to replicate sit in file drawers?
    
# Overcoming these weaknesses

## Performing research

- Protecting against cognitive biases
- Improving methodological training
- Implementing independent methological support
- Encouraging collaboration and team science
- Collect bigger samples

## Reporting on research

- Promoting study pre-registration
    - Registered reports ([Munafo et al. 2017](https://doi.org/10.1038/s41562-016-0021), Box 3)
- Improving the quality of reporting
    - [The Transparency and Openness Promotion (TOP) guidelines](https://osf.io/ud578/) and [signatories](https://cos.io/top/#list)

## Reporting on research

- Franco, A., Malhotra, N., & Simonovits, G. (2016). Underreporting in Psychology Experiments: Evidence From a Study Registry. Social Psychological and Personality Science, 7(1), 8–12. https://doi.org/10.1177/1948550615598377 
- "*We find that about 40% of studies fail to fully report all experimental conditions and about 70% of studies do not report all outcome variables included in the questionnaire. Reported effect sizes are about twice as large as unreported effect sizes and are about 3 times more likely to be statistically significant.*"

## Reporting on research

- Publish replications

## Verifying research

- Promoting transparency and open science
- Open methods, materials, code sharing, data sharing, 

## Changing Incentives

- Higginson, A. D., & Munafò, M. R. (2016). Current Incentives for Scientists Lead to Underpowered Studies with Erroneous Conclusions. PLOS Biology, 14(11), e2000995. https://doi.org/10.1371/journal.pbio.2000995
  - Claim that current publication incentive structure reinforces current practices
- OSF badge system
- Other incentives/disincentives

## Status report/recommendations by stakeholder group

<iframe src="http://www.nature.com/articles/s41562-016-0021/tables/1">
</iframe>
Source: <http://www.nature.com/articles/s41562-016-0021/tables/1>

# Your thoughts?

## Questions for discussion

- Which of the manifesto provisions would you disagree with?
- Do you agree with the assessment about progress ([Table 1](http://www.nature.com/articles/s41562-016-0021/tables/1))
- What steps could **you** take?